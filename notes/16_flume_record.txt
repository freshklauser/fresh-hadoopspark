# 1. 配置文件(安装环境java配置)
cp  flume-env.sh.template  flume-env.sh
vim conf/flume-env.sh
JAVA_HOME=/usr/local/java

# >>>>>>>>>>>>>>>>>>>>>>> log-flume-kafka <<<<<<<<<<<<<<<<<<<<<<<<<<<
# 2. sources,channels,sinks配置
cd  /usr/local/flume/conf    
vim log-kafka.properties

	[centos@localhost flume]$ cat conf/log-kafka.properties 

	# >>>>>>>>>>  Name the components on this agent <<<<<<<<<<
	agent.sources = exectail
	agent.channels = memoryChannel
	agent.sinks = kafkasink

	# >>>>>>>>>> Describe/configure the source <<<<<<<<<<<
	agent.sources.exectail.type = exec
	# 下面这个路径是需要收集日志的绝对路径，改为自己的日志目录
	# tail -f <file> : -f 表示循环读取， 循环读取<file>最后10行内容
	agent.sources.exectail.command = tail -f /home/centos/flume_logs/agent.log
	# 拦截器？？
	agent.sources.exectail.interceptors = i1
	agent.sources.exectail.interceptors.i1.type = regex_filter
	# 定义日志过滤前缀的正则
	agent.sources.exectail.interceptors.i1.regex = .+PRODUCT_RATING_PREFIX.+

	# >>>>>>>>>> define channels <<<<<<<<<<<
	agent.channels.memoryChannel.type = memory
	agent.channels.memoryChannel.capacity = 10000 

	# >>>>>>>>>> define sinks <<<<<<<<<<<
	agent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
	agent.sinks.kafkasink.kafka.topic = log
	agent.sinks.kafkasink.kafka.bootstrap.servers = 192.168.1.213:6667
	agent.sinks.kafkasink.kafka.producer.acks = 1
	agent.sinks.kafkasink.kafka.flumeBatchSize = 20

	# >>>>>>>>>> Bind the sources and sinks to the channels  <<<<<<<<<<<
	agent.sources.exectail.channels = memoryChannel
	agent.sinks.kafkasink.channel = memoryChannel
	# tips: source:channel = 1:n ; sink:chanel=1:1, 因此source用channels, sink用channel




# 3. 启动flume
[centos@localhost flume]$ ./bin/flume-ng agent -c ./conf/ -f ./conf/log-kafka.properties -n agent -Dflume.root.logger=INFO,console
	-n agent: 指定agent名 ，-n 	Agent的名称（必填）
	-c ./conf/: 在目录使用配置文件。指定配置文件放在什么目录, 也可以 --conf
	-f ./conf/log-kafka.properties：指定配置文件，这个配置文件必须在全局选项的–conf参数定义的目录下。（必填）
	-Dflume.root.logger=INFO,console：将日志输入到控制台上
	
	
# >>>>>>>>>>>>>>>> windows目录挂载到linux <<<<<<<<<<<<<<<<<
由于测试环境下 埋点日志 在windows系统下，flume在linux系统下，flume的source监控的日志需要先挂载到linux
todo: 埋点日志除了设置本地外，也可以直接设置为hadoop路径(应该需要复制hadoop的属性文件到resources中)
# 1.
	businessServer的resources中log4j日志，设置埋点日志本地路径：
	G:\demo\DEMO_2\ECommerceRecommendSystem\businessServer\src\main\log\agent.log
# 2.
1). 先在 Windows 下面共享需要挂载的目录
    右键需要共享的文件夹 , 选择共享 , 然后默认设置就可以
    然后在其他的机器测试是否能正常访问 , 在其他机器资源管理器里面输入 \\xxx.xxx.xxx.xxx\Share , 地址格式是 \\你的IP\你的共享文件夹
    出现无法访问或者指定的密码不正确时 , 可参考如下设置 :
    修改网络安全设置 : 开始 -> 运行 gpedit.msc , 打开组策略 -> 计算机配置 -> Windows设置 -> 安全设置 -> 本地策略 -> 安全选项 -> 
	选择 “网络安全 : LAN 管理器身份验证级别” , 双击打开 , 设置成“发送 LM 和 NTLM 响应” ;
    修改网络访问模型 : 开始 -> 运行 -> gpedit.msc -> 计算机配置 -> Windows设置 -> 安全设置 -> 本地策略 -> 安全选项 -> 选择 “网络访问:本地帐户的共享和安全模型” , 修改为使用经典模式 ;

2). linux下挂载windows的共享目录：
	[centos@master ~]$ sudo mount -t cifs -o username=Administrator,password=szlh.2020 //10.180.164.137/demo /mnt/G/demo/
	df -h # 查看挂载情况   不要用 -lh

3). 卸载挂载：
	umount /usr/local/bin/code
	
	
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> flum course <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
1. netcat 安裝和使用
源码编译安裝：
	下载地址：http://sourceforge.net/projects/netcat/files/netcat/0.7.1/（下载的是netcat-0.7.1.tar.gz版本）
	wget https://sourceforge.net/projects/netcat/files/netcat/0.7.1/netcat-0.7.1.tar.gz
	sudo tar -zxvf netcat-0.7.1.tar.gz -C /usr/local
	sudo mv netcat-0.7.1 netcat
	cd /usr/local/netcat
	# 配置，将安装文件放在 /opt/netcat 目录下，卸载时，只要删除该目录即可
	./configure --prefix=/opt/netcat
	# 编译安装, 之后会创建 /opt/netcat 安装文件夹
	sudo make
	sudo make install
	# 添加环境配置
	su root
	vim /etc/profile
		# set  netcat path
		export NETCAT_HOME=/opt/netcat
		export PATH=$PATH:$NETCAT_HOME/bin
	# 使配置生效
	source /etc/profile
	# 检验安装是否成功
	nc --help
	# 退出root用戶
	
监听端口4444是否被占用：
	sudo netcat -ntulp | grep 4444
	
2. netcat 参数
	参数 	作用
	-g 	<网关> 设置路由器跃程通信网关，最多可设置8个
	-G  指向器数目 	设置来源路由指向器，其数值为4的倍数
	-h 	在线帮助；
	-i  延迟秒数 	设置时间间隔，以便传送信息及扫描通信端口；
	-l 	使用监听模式，管控传入的资料；
	-n 	直接使用IP地址，而不通过域名服务器；
	-o  输出文件 	指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存；
	-p  通信端口 	设置本地主机使用的通信端口；
	-r 	乱数指定本地与远端主机的通信端口；
	-s  来源位址 	设置本地主机送出数据包的IP地址；
	-u 	使用UDP传输协议扫描，不加默认为TCP扫描；
	-v 	显示指令执行过程；
	-w  超时秒数 	设置等待连线的时间；
	-z 	使用0输入/输出模式，只在扫描通信端口时使用。
	-e 	program to exec after connect
	-c 	as `-e’; use /bin/sh to exec

	
3. 聊天
发送端：侦听服务器
	[centos@node4 ~]$ nc -l -p 5522
	hello
	st
	...
	注：-l 侦听，-p 指定端口号
接收端：
	[centos@node5 ~]$ nc -nv 192.168.1.214 5522
	192.168.1.214 5522 open
	hello
	st
	
4.  >>>>>>>>>>>>>>>>>>>>>>> netcat-flume-logger <<<<<<<<<<<<<<<<<<<<<<<<<<<
netcat-flume-logger.properties
	# Name the componets on this agent: a1
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Decribe/configure the source
	a1.sources.r1.type = netcat
	a1.sources.r1.bind = localhost
	a1.sources.r1.port = 4444

	# Describe the sink 
	a1.sinks.k1.type = logger

	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel 
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
启动flume:
	[centos@node5 flume]$ ./bin/flume-ng agent -n a1 -c ./conf/ -f ./job/netcat-flume-log.properties -Dflume.root.logger=INFO,console
启动(flume先启动，再开启nc, 发送端):
	[centos@node5 flume]$ nc localhost 4444
注意：
	1) a1.sources.r1.bind = localhost 或 a1.sources.r1.bind = <本机ip>, 貌似只能是本机的netcat, 其他ip无法bind
	

5. >>>>>>>>>>>>>>>>>>>>>>> netcat-flume-logger <<<<<<<<<<<<<<<<<<<<<<<<<<<



6. >>>>>>>>>>>>>>>>>>>>>>> log-flume-hdfs <<<<<<<<<<<<<<<<<<<<<<<<<<<
前提：hadoop相关jar包需要拷贝到 flume/lib中
	commons-configuration-1.6.jar、
	hadoop-auth-2.7.2.jar、
	hadoop-common-2.7.2.jar、
	hadoop-hdfs-2.7.2.jar、
	commons-io-2.4.jar、
	htrace-core-3.1.0-incubating.jar

log-flume-hdfs.conf:
	# Name the components on this agent
	a2.sources = r2
	a2.sinks = k2
	a2.channels = c2


	# Describe/configure the source
	a2.sources.r2.type = exec
	a2.sources.r2.command = tail -F /mnt/G/demo/DEMO_2/ECommerceRecommendSystem/businessServer/src/main/log/agent.log
	a2.sources.r2.shell = /bin/bash -c


	# Describe the sink
	a2.sinks.k2.type = hdfs
	a2.sinks.k2.hdfs.path = hdfs://master:9000/lyu/flume/%Y%m%d/%H
	# 上传文件的前缀
	a2.sinks.k2.hdfs.filePrefix = logs-
	# hive中一天数据一个分区？滚动文件夹设置为以天为单位，则一天的数据在一个文件夹中，很方便load数据对应hive一个分区？

	# 是否按照时间滚动《文件夹》
	a2.sinks.k2.hdfs.round = true
	# 多少时间单位创建一个新的文件夹
	a2.sinks.k2.hdfs.roundValue = 1
	# 重新定义时间单位
	a2.sinks.k2.hdfs.roundUnit = hour		# flume/%Y%m%d/%H，这里是H，对应设置 roundUnit=hour

	# 是否使用本地时间戳，这里必须有
	a2.sinks.k2.hdfs.useLocalTimeStamp = true
	
	# key: 必须
	a2.sinks.k2.hdfs.minBlockReplicas=1

	# 批大小：积攒多少个 Event 才 flush 到 HDFS 一次
	a2.sinks.k2.hdfs.batchSize = 1000

	# 设置文件类型，可支持压缩
	a2.sinks.k2.hdfs.fileType = DataStream

	# -------------- 以下3个满足其中之一就会生成新文件 ------------
	# 多久生成一个新的《文件》，单位：秒, 生产中不能设置这么短时间，小文件太多
	a2.sinks.k2.hdfs.rollInterval = 30
	# 设置每个文件的滚动大小，一般生产中设置为 hdfs 的块大小,比块大小略小一些(128M)
	a2.sinks.k2.hdfs.rollSize = 134217700
	# 文件的滚动与 Event 数量无关, 生产中一般关闭rollCount
	a2.sinks.k2.hdfs.rollCount = 0


	# Use a channel which buffers events in memory
	a2.channels.c2.type = memory
	a2.channels.c2.capacity = 1000
	a2.channels.c2.transactionCapacity = 100


	# Bind the source and sink to the channel
	a2.sources.r2.channels = c2
	a2.sinks.k2.channel = c2
	
启动fluem运行agent:a2
	[centos@master flume]$ ./bin/flume-ng agent -n a2 -c ./conf/ -f ./job/log-flume-hdfs.conf
hdfs文件系统中出现文件夹：/lyu/flume/20200114/03

可能出现的问题 1：
	设置的文件滚动参数不起作用，一直在产生小文件
解决方案：
	增加如下flume sink配置项， refer: https://blog.csdn.net/simonchi/article/details/43231891
	a2.sinks.k2.hdfs.minBlockReplicas=1


7. >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> dir-flume-hdfs <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
dir-flume-hdfs.conf:
# Name the components on this agent
	a3.sources = r3
	a3.sinks = k3
	a3.channels = c3


	# Describe/configure the source
	a3.sources.r3.type = spooldir
	a3.sources.r3.spoolDir = /mnt/D/output
	a3.sources.r3.fileSuffix = .COMPLITED
	a3.sources.r3.fileHeader = true
	a3.sources.r3.ignorePattern = ([^ ]*\.tmp)

	# 是否按照时间滚动《文件夹》
	a3.sinks.k3.hdfs.round = true
	# 多少时间单位创建一个新的文件夹
	a3.sinks.k3.hdfs.roundValue = 1
	# 重新定义时间单位
	a3.sinks.k3.hdfs.roundUnit = hour		# flume/%Y%m%d/%H，这里是H，对应设置 roundUnit=hour

	# 是否使用本地时间戳
	a3.sinks.k3.hdfs.useLocalTimeStamp = true

	# key,if not set to 1, samll file will created and roll_params do not work
	a3.sinks.k3.hdfs.minBlockReplicas=1				# 必须制定，否则还是会使文件滚动参数无效，产生很多小文件

	# 批大小：积攒多少个 Event 才 flush 到 HDFS 一次
	a3.sinks.k3.hdfs.batchSize = 1000

	# 设置文件类型，可支持压缩
	a3.sinks.k3.hdfs.fileType = DataStream

	# -------------- 以下3个满足其中之一就会生成新文件 ------------
	# 如果同时配置了时间策略rollInterval和文件大小策略，那么会先判断时间，如果时间没到再判断其他的条件
	# 多久生成一个新的《文件》，单位：秒, 生产中不能设置这么短时间，小文件太多
	a3.sinks.k3.hdfs.rollInterval = 30
	# 设置每个文件的滚动大，一般生产中设置为 hdfs 的块大小,比块大小略小一些(128M)
	a3.sinks.k3.hdfs.rollSize = 134217700
	# 文件的滚动与 Event 数量无关, 生产中一般关闭rollCount
	a3.sinks.k3.hdfs.rollCount = 0


	# Use a channel which buffers events in memory
	a3.channels.c3.type = memory
	a3.channels.c3.capacity = 1000
	a3.channels.c3.transactionCapacity = 100


	# Bind the source and sink to the channel
	a3.sources.r3.channels = c3
	a3.sinks.k3.channel = c3

NOTES:
1) 不能监控动态变化的数据(即文件夹内的文件内容不能动态修改，否则需要删除动态修改的文件，重启flume)
2) 添加的后缀名不要使用预计上传的文件的后缀名
3) 监控目录中文件名重复会报错，上传的时候不会报错，改名字加后缀会报错，导致flume-ng不能正常运行
4) 被监控文件目录《每500毫秒》扫描一次文件变动
5) 扫描上传后的多个文件内容写入同一个hdfs的文件中存储，而不是按照原扫描目录下的文件一个一个存储在hdfs中
6) 在flume中采集的一行数据量是有大小限制的，默认为2048，即2KB,超过这个限制会被切分成多行
   -- refer:https://www.cnblogs.com/breg/p/5649363.html
   可以看看refer中推荐的第二种解决方案
   解析：
		Source使用嵌入式的反序列化器读取监控目录下的文件(这里以Spooling Directory Source为例)，默认的反序列化器是LineDeserializer。
		该反序列化器会《按行读取文件中的内容，封装成一个Event消息》。默认一次读取的最大长度是2048个字符。
		可以设置source的如下参数设置maxLineLength:
			a3.sources.r3.deserializer.maxLineLength = 204800
	---> 一行文本内容被反序列化成一个event。event的最大定义为2048字节，超过会被切割放到下一个event中。
7) 《enventBased策略下》，上传文件大小sizeUploadFileLocal 与 hdfs分割文件数量numFileCutdHDFS 关系：
	（timeBased策略下,rollCount=0，禁用）
	上传文件大小 sizeUploadFileLocal = 522k
	参数设置：
		maxLineLength = 204800  # 20k
		rollCount = 2
	hdfs分割文件单个大小 sizePerFileHDFS = maxLineLength * rollCount 
		if sizePerFileHDFS > siezeUploadFileLocal:
			numFileCutdHDFS = 1
			sizeFileHDFS = sizePerFileHDFS
		else:
			numFileCutdHDFS = math.ceil(sizeUploadFileLocal / sizePerFileHDFS)
			sizeFileListsHDFS[0:-1] = sizePerFileHDFS
			sizeFileListsHDFS[-1] = sizeUploadFileLocal - sizePerFileHDFS * (numFileCutdHDFS - 1)
	eg.
	sizeUploadFileLocal = 522k, maxLineLength = 20k, rollCount = 2  --->	numFileCutdHDFS = 14 (13*40k + 2.15k)
	sizeUploadFileLocal = 522k, maxLineLength = 20k, rollCount = 20 --->	numFileCutdHDFS = 2  (400k + 122.5k)
	......
8) hdfs需要event的header里要带时间戳，如果没有可以通过设置hdfs.useLocalTimeStamp=true来使用本地时间戳


PROBLEMS:
1) windows共享目录挂载到linux后，由于权限问题，flume-ng监控会出租哦，permission denied, 将共享文件夹的文件转移到centos7的路径后，ok!

UPDATE:
1) 实现在每天的0点滚动文件
	refer: https://blog.csdn.net/weixin_33738578/article/details/86025675
	由于官方的1.8版本hdfs-sink不能在每天的0点滚动文件，所以修改了flume-hdfs-sink源码。
	flume-hdfs-sink中修改了HDFSEventSink.java文件，其他文件未改动.
	timeRollerFlag:
		默认值： day,可以设置minutes, hour, day .minutes 每分钟滚动文件,hour 每小时滚动文件,day 每天0点滚动文件
	hdfsSink的配置如下：
		paas2.sinks.k1.type = hdfs
		paas2.sinks.k1.hdfs.path = hdfs://ns1/user/hive/warehouse/dw_stg.db/tg_paas_business/dt=%Y%m%d
		paas2.sinks.k1.hdfs.filePrefix = paas_business_%Y%m%d
		paas2.sinks.k1.hdfs.fileSuffix = .COMPLITED
		paas2.sinks.k1.hdfs.rollInterval = 0
		paas2.sinks.k1.hdfs.rollCount = 0
		paas2.sinks.k1.hdfs.round = false
		paas2.sinks.k1.hdfs.roundValue=12
		paas2.sinks.k1.hdfs.roundUnit=hour
		paas2.sinks.k1.hdfs.rollSize = 134217728
		paas2.sinks.k1.hdfs.fileType = DataStream
		paas2.sinks.k1.hdfs.timeRollerFlag=day		# 滚动策略，每天0点滚动文件

	编译后的jar包flume-hdfs-sink-1.8.0.jar.modify_0000_rollFile.bak 替换flume/lib下的 flume-hdfs-sink-1.8.0.jar
	
	