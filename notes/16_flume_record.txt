# 1. 配置文件(安装环境java配置)
cp  flume-env.sh.template  flume-env.sh
vim conf/flume-env.sh
JAVA_HOME=/usr/local/java

# >>>>>>>>>>>>>>>>>>>>>>> log-flume-kafka <<<<<<<<<<<<<<<<<<<<<<<<<<<
# 2. sources,channels,sinks配置
cd  /usr/local/flume/conf    
vim log-kafka.properties

	[centos@localhost flume]$ cat conf/log-kafka.properties 

	# >>>>>>>>>>  Name the components on this agent <<<<<<<<<<
	agent.sources = exectail
	agent.channels = memoryChannel
	agent.sinks = kafkasink

	# >>>>>>>>>> Describe/configure the source <<<<<<<<<<<
	agent.sources.exectail.type = exec
	# 下面这个路径是需要收集日志的绝对路径，改为自己的日志目录
	# tail -f <file> : -f 表示循环读取， 循环读取<file>最后10行内容
	agent.sources.exectail.command = tail -f /home/centos/flume_logs/agent.log
	# 拦截器？？
	agent.sources.exectail.interceptors = i1
	agent.sources.exectail.interceptors.i1.type = regex_filter
	# 定义日志过滤前缀的正则
	agent.sources.exectail.interceptors.i1.regex = .+PRODUCT_RATING_PREFIX.+

	# >>>>>>>>>> define channels <<<<<<<<<<<
	agent.channels.memoryChannel.type = memory
	agent.channels.memoryChannel.capacity = 10000 

	# >>>>>>>>>> define sinks <<<<<<<<<<<
	agent.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
	agent.sinks.kafkasink.kafka.topic = log
	agent.sinks.kafkasink.kafka.bootstrap.servers = 192.168.1.213:6667
	agent.sinks.kafkasink.kafka.producer.acks = 1
	agent.sinks.kafkasink.kafka.flumeBatchSize = 20

	# >>>>>>>>>> Bind the sources and sinks to the channels  <<<<<<<<<<<
	agent.sources.exectail.channels = memoryChannel
	agent.sinks.kafkasink.channel = memoryChannel
	# tips: source:channel = 1:n ; sink:chanel=1:1, 因此source用channels, sink用channel




# 3. 启动flume
[centos@localhost flume]$ ./bin/flume-ng agent -c ./conf/ -f ./conf/log-kafka.properties -n agent -Dflume.root.logger=INFO,console
	-n agent: 指定agent名 ，-n 	Agent的名称（必填）
	-c ./conf/: 在目录使用配置文件。指定配置文件放在什么目录, 也可以 --conf
	-f ./conf/log-kafka.properties：指定配置文件，这个配置文件必须在全局选项的–conf参数定义的目录下。（必填）
	-Dflume.root.logger=INFO,console：将日志输入到控制台上
	
	
# >>>>>>>>>>>>>>>> windows目录挂载到linux <<<<<<<<<<<<<<<<<
由于测试环境下 埋点日志 在windows系统下，flume在linux系统下，flume的source监控的日志需要先挂载到linux
todo: 埋点日志除了设置本地外，也可以直接设置为hadoop路径(应该需要复制hadoop的属性文件到resources中)
# 1.
	businessServer的resources中log4j日志，设置埋点日志本地路径：
	G:\demo\DEMO_2\ECommerceRecommendSystem\businessServer\src\main\log\agent.log
# 2.
1). 先在 Windows 下面共享需要挂载的目录
    右键需要共享的文件夹 , 选择共享 , 然后默认设置就可以
    然后在其他的机器测试是否能正常访问 , 在其他机器资源管理器里面输入 \\xxx.xxx.xxx.xxx\Share , 地址格式是 \\你的IP\你的共享文件夹
    出现无法访问或者指定的密码不正确时 , 可参考如下设置 :
    修改网络安全设置 : 开始 -> 运行 gpedit.msc , 打开组策略 -> 计算机配置 -> Windows设置 -> 安全设置 -> 本地策略 -> 安全选项 -> 
	选择 “网络安全 : LAN 管理器身份验证级别” , 双击打开 , 设置成“发送 LM 和 NTLM 响应” ;
    修改网络访问模型 : 开始 -> 运行 -> gpedit.msc -> 计算机配置 -> Windows设置 -> 安全设置 -> 本地策略 -> 安全选项 -> 选择 “网络访问:本地帐户的共享和安全模型” , 修改为使用经典模式 ;

2). linux下挂载windows的共享目录：
	[centos@master ~]$ sudo mount -t cifs -o username=Administrator,password=szlh.2020 //10.180.164.137/demo /mnt/G/demo/
	df -h # 查看挂载情况   不要用 -lh

3). 卸载挂载：
	umount /usr/local/bin/code
	
	
# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> flum course <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
1. netcat 安裝和使用
源码编译安裝：
	下载地址：http://sourceforge.net/projects/netcat/files/netcat/0.7.1/（下载的是netcat-0.7.1.tar.gz版本）
	wget https://sourceforge.net/projects/netcat/files/netcat/0.7.1/netcat-0.7.1.tar.gz
	sudo tar -zxvf netcat-0.7.1.tar.gz -C /usr/local
	sudo mv netcat-0.7.1 netcat
	cd /usr/local/netcat
	# 配置，将安装文件放在 /opt/netcat 目录下，卸载时，只要删除该目录即可
	./configure --prefix=/opt/netcat
	# 编译安装, 之后会创建 /opt/netcat 安装文件夹
	sudo make
	sudo make install
	# 添加环境配置
	su root
	vim /etc/profile
		# set  netcat path
		export NETCAT_HOME=/opt/netcat
		export PATH=$PATH:$NETCAT_HOME/bin
	# 使配置生效
	source /etc/profile
	# 检验安装是否成功
	nc --help
	# 退出root用戶
	
监听端口4444是否被占用：
	sudo netcat -ntulp | grep 4444
	
2. netcat 参数
	参数 	作用
	-g 	<网关> 设置路由器跃程通信网关，最多可设置8个
	-G  指向器数目 	设置来源路由指向器，其数值为4的倍数
	-h 	在线帮助；
	-i  延迟秒数 	设置时间间隔，以便传送信息及扫描通信端口；
	-l 	使用监听模式，管控传入的资料；
	-n 	直接使用IP地址，而不通过域名服务器；
	-o  输出文件 	指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存；
	-p  通信端口 	设置本地主机使用的通信端口；
	-r 	乱数指定本地与远端主机的通信端口；
	-s  来源位址 	设置本地主机送出数据包的IP地址；
	-u 	使用UDP传输协议扫描，不加默认为TCP扫描；
	-v 	显示指令执行过程；
	-w  超时秒数 	设置等待连线的时间；
	-z 	使用0输入/输出模式，只在扫描通信端口时使用。
	-e 	program to exec after connect
	-c 	as `-e’; use /bin/sh to exec

	
3. 聊天
发送端：侦听服务器
	[centos@node4 ~]$ nc -l -p 5522
	hello
	st
	...
	注：-l 侦听，-p 指定端口号
接收端：
	[centos@node5 ~]$ nc -nv 192.168.1.214 5522
	192.168.1.214 5522 open
	hello
	st
	
4.  >>>>>>>>>>>>>>>>>>>>>>> netcat-flume-logger <<<<<<<<<<<<<<<<<<<<<<<<<<<
netcat-flume-logger.properties
	# Name the componets on this agent: a1
	a1.sources = r1
	a1.sinks = k1
	a1.channels = c1

	# Decribe/configure the source
	a1.sources.r1.type = netcat
	a1.sources.r1.bind = localhost
	a1.sources.r1.port = 4444

	# Describe the sink 
	a1.sinks.k1.type = logger

	# Use a channel which buffers events in memory
	a1.channels.c1.type = memory
	a1.channels.c1.capacity = 1000
	a1.channels.c1.transactionCapacity = 100

	# Bind the source and sink to the channel 
	a1.sources.r1.channels = c1
	a1.sinks.k1.channel = c1
启动flume:
	[centos@node5 flume]$ ./bin/flume-ng agent -n a1 -c ./conf/ -f ./job/netcat-flume-log.properties -Dflume.root.logger=INFO,console
启动(flume先启动，再开启nc, 发送端):
	[centos@node5 flume]$ nc localhost 4444
注意：
	1) a1.sources.r1.bind = localhost 或 a1.sources.r1.bind = <本机ip>, 貌似只能是本机的netcat, 其他ip无法bind
	

5. >>>>>>>>>>>>>>>>>>>>>>> netcat-flume-logger <<<<<<<<<<<<<<<<<<<<<<<<<<<



6. >>>>>>>>>>>>>>>>>>>>>>> log-flume-hdfs <<<<<<<<<<<<<<<<<<<<<<<<<<<
前提：hadoop相关jar包需要拷贝到 flume/lib中
	commons-configuration-1.6.jar、
	hadoop-auth-2.7.2.jar、
	hadoop-common-2.7.2.jar、
	hadoop-hdfs-2.7.2.jar、
	commons-io-2.4.jar、
	htrace-core-3.1.0-incubating.jar

log-flume-hdfs.conf:
	# Name the components on this agent
	a2.sources = r2
	a2.sinks = k2
	a2.channels = c2


	# Describe/configure the source
	a2.sources.r2.type = exec
	a2.sources.r2.command = tail -F /mnt/G/demo/DEMO_2/ECommerceRecommendSystem/businessServer/src/main/log/agent.log
	a2.sources.r2.shell = /bin/bash -c


	# Describe the sink
	a2.sinks.k2.type = hdfs
	a2.sinks.k2.hdfs.path = hdfs://master:9000/lyu/flume/%Y%m%d/%H
	# 上传文件的前缀
	a2.sinks.k2.hdfs.filePrefix = logs-
	
	# 控制分区：hive中一天数据一个分区？滚动文件夹设置为以天为单位，则一天的数据在一个文件夹中，很方便load数据对应hive一个分区？
	# 是否按照时间滚动《文件夹》
	a2.sinks.k2.hdfs.round = true
	# 多少时间单位创建一个新的文件夹
	a2.sinks.k2.hdfs.roundValue = 1
	# 重新定义时间单位
	a2.sinks.k2.hdfs.roundUnit = hour		# flume/%Y%m%d/%H，这里是H，对应设置 roundUnit=hour

	# 是否使用本地时间戳，这里必须有
	a2.sinks.k2.hdfs.useLocalTimeStamp = true
	
	# key: 必须
	a2.sinks.k2.hdfs.minBlockReplicas=1

	# 批大小：积攒多少个 Event 才 flush 到 HDFS 一次
	a2.sinks.k2.hdfs.batchSize = 1000

	# 设置文件类型，可支持压缩
	a2.sinks.k2.hdfs.fileType = DataStream

	# -------------- 以下3个满足其中任何之一就会生成新文件 ------------
	# 多久生成一个新的《文件》，单位：秒, 生产中不能设置这么短时间，小文件太多，一般设600s即10分钟滚动一次文件
	a2.sinks.k2.hdfs.rollInterval = 30
	# 设置每个文件的滚动大小(字节)，一般生产中设置为 hdfs 的块大小,比块大小略小一些(128M)
	a2.sinks.k2.hdfs.rollSize = 134217700
	# 文件的滚动与 Event 数量无关, 生产中一般关闭rollCount
	a2.sinks.k2.hdfs.rollCount = 0


	# Use a channel which buffers events in memory
	a2.channels.c2.type = memory
	a2.channels.c2.capacity = 1000
	a2.channels.c2.transactionCapacity = 100


	# Bind the source and sink to the channel
	a2.sources.r2.channels = c2
	a2.sinks.k2.channel = c2
	
启动fluem运行agent:a2
	[centos@master flume]$ ./bin/flume-ng agent -n a2 -c ./conf/ -f ./job/log-flume-hdfs.conf
hdfs文件系统中出现文件夹：/lyu/flume/20200114/03

可能出现的问题 1：
	设置的文件滚动参数不起作用，一直在产生小文件
解决方案：
	增加如下flume sink配置项， refer: https://blog.csdn.net/simonchi/article/details/43231891
	a2.sinks.k2.hdfs.minBlockReplicas=1


7. >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> dir-flume-hdfs <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
dir-flume-hdfs.conf:
	# Name the components on this agent
	a3.sources = r3
	a3.sinks = k3
	a3.channels = c3


	# Describe/configure the source
	a3.sources.r3.type = spooldir
	#a3.sources.r3.spoolDir = /mnt/D/output
	a3.sources.r3.spoolDir = /usr/local/flume/lyu
	a3.sources.r3.fileSuffix = .COMPLITED
	a3.sources.r3.fileHeader = true
	a3.sources.r3.ignorePattern = ([^ ]*\.tmp)

	# Describe the sink
	a3.sinks.k3.type = hdfs
	a3.sinks.k3.hdfs.path = hdfs://master:9000/lyu/flume/%Y%m%d/%H
	# 上传文件的前缀
	a3.sinks.k3.hdfs.filePrefix = time-based-upload-

	# 是否启用时间上的”舍弃”，这里的”舍弃”，类似于”四舍五入”，比如17:28时间生产的文件夹，命名为1720
	# 影响除了%t的其他所有时间表达式,通常用在目录级别，控制文件夹的创建
	a3.sinks.k3.hdfs.round = true
	# 多少时间单位创建一个新的文件夹
	a3.sinks.k3.hdfs.roundValue = 1
	# 重新定义时间单位
	a3.sinks.k3.hdfs.roundUnit = hour
	# flume/%Y%m%d/%H，这里是H，对应设置 roundUnit=hour

	# 是否使用本地时间戳
	a3.sinks.k3.hdfs.useLocalTimeStamp = true

	# key,if not set to 1, samll file will created and roll_params do not work
	# 必须指定，否则还是会使文件滚动参数无效，产生很多小文件
	a3.sinks.k3.hdfs.minBlockReplicas=1

	# 批大小：积攒多少个 Event 才 flush 到 HDFS 一次
	a3.sinks.k3.hdfs.batchSize = 1000

	# 设置文件类型，可支持压缩
	a3.sinks.k3.hdfs.fileType = DataStream

	# -------------- 以下3个满足其中任意之一就会生成新文件 ------------
	# 多久生成一个新的《文件》，单位：秒, 生产中不能设置这么短时间，小文件太多
	a3.sinks.k3.hdfs.rollInterval = 30
	# 设置每个文件的滚动大，一般生产中设置为 hdfs 的块大小,比块大小略小一些(128M)
	a3.sinks.k3.hdfs.rollSize = 134217700
	# 文件的滚动与 Event 数量无关, 生产中一般关闭rollCount
	a3.sinks.k3.hdfs.rollCount = 0

	# 设置闲置时间600s，达到时长没数据的话就自动关闭文件
	a3.sinks.k3.hdfs.idleTimeout = 600

	# Use a channel which buffers events in memory
	a3.channels.c3.type = memory
	a3.channels.c3.capacity = 1000
	a3.channels.c3.transactionCapacity = 100


	# Bind the source and sink to the channel
	a3.sources.r3.channels = c3
	a3.sinks.k3.channel = c3


8 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> flume监控 <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
监控工具：ganglia
	安装：
		sudo yum -y install httpd php
		sudo yum -y install rrdtool perl-rrdtool
		sudo yum -y install apr-devel
		sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
		sudo yum -y install ganglia-gmetad
		sudo yum -y install ganglia-web
		sudo yum -y install ganglia-gmond
	配置：
	1) [klaus@messi ~]$ sudo vim /etc/httpd/conf.d/ganglia.conf: 改为如下内容 Require all granted
		Alias /ganglia /usr/share/ganglia
		<Location /ganglia>
		  #  Require local
		  Require all granted
		  # Require ip 10.1.2.3
		  # Require host example.org
		</Location>
	2) [klaus@messi ~]$ sudo vim /etc/ganglia/gmetad.conf
		data_source "messi" 192.168.13.128			
		# 这里"messi"可以随意指定，后面可以跟多个ip或Hostname,每一个ip/hosname是一个监控的节点
	3) [klaus@messi ~]$ sudo vim /etc/ganglia/gmond.conf 
		cluster {
			name = "messi"					#--- 更改为自己的hostname, 与2)中的 data_source匹配
			owner = "unspecified"
			latlong = "unspecified"
			url = "unspecified"
			}

	
		# mcast_join = 239.2.11.71		    #--- 注释掉
		host = 192.168.13.128				#--- 新增
		port = 8649
		ttl = 1
		...
		udp_recv_channel {
			# mcast_join = 239.2.11.71		#--- 注释掉
			port = 8649
			bind = 192.168.13.128			#--- 新增
			retry_bind = true
	4) sudo subl /etc/selinux/config (selinux 本次生效关闭必须重启； 临时关闭：sudo setenforce 0)
		# disabled - No SELinux policy is loaded.
		SELINUX=disabled
	启动：
		sudo systemctl start httpd.service
		sudo systemctl start gmetad.service 
	    sudo systemctl start gmond.service
	启动状态：
		sudo systemctl status httpd.service 
		sudo systemctl status gmetad.service 
		sudo systemctl status gmond.service 

配置flume监控：
	1) 配置flume-env.sh，添加JAVA_OPTS内容如下：
	JAVA_OPTS="-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.13.128:8649 -Xms100m -Xmx200m"
	2) 启动flume-ng时添加monitoring参数如下：
		./bin/flume-ng agent -c conf/ -f job/group1/taildir-flume-avros_replicating.conf -n a1 -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.13.128:8649

监控主页：
	http://192.168.13.128/ganglia/
	
监控主要内容(flume metrics -- 47)：
	字段（图表名称） 				字段含义
	EventPutAttemptCount 		source 尝试写入 channel 的事件总数量
	EventPutSuccessCount 		成功写入 channel 且提交的事件总数量
	EventTakeAttemptCount 		sink 尝试从 channel 拉取事件的总数量。
	EventTakeSuccessCount 		sink 成功读取的事件的总数量
	StartTime 					channel 启动的时间（毫秒）
	StopTime 					channel 停止的时间（毫秒）
	channelSize 				目前 channel 中事件的总数量
	channelFillPercentage 		channel占用百分比
	channelCapacity 			channel的容量
	其中：如果 EventPutSuccessCount = channelCapacity + EventTakeSuccessCount, 说明没有丢数据;
	如果后者之后小于前者，则有数据丢失

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> NOTES <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
NOTES:
1) 不能监控动态变化的数据(即文件夹内的文件内容不能动态修改，否则需要删除动态修改的文件，重启flume)

2) 添加的后缀名不要使用预计上传的文件的后缀名

3) 监控目录中文件名重复会报错，上传的时候不会报错，改名字加后缀会报错，导致flume-ng不能正常运行

4) 被监控文件目录《每500毫秒》扫描一次文件变动

5) 扫描上传后的多个文件内容写入同一个hdfs的文件中存储，而不是按照原扫描目录下的文件一个一个存储在hdfs中

6) 在flume中采集的一行数据量是有大小限制的，默认为2048，即2KB,超过这个限制会被切分成多行
   -- refer:https://www.cnblogs.com/breg/p/5649363.html
   可以看看refer中推荐的第二种解决方案
   解析：
		Source使用嵌入式的反序列化器读取监控目录下的文件(这里以Spooling Directory Source为例)，默认的反序列化器是LineDeserializer。
		该反序列化器会《按行读取文件中的内容，封装成一个Event消息》。默认一次读取的最大长度是2048个字符。
		可以设置source的如下参数设置maxLineLength:
			a3.sources.r3.deserializer.maxLineLength = 204800
	---> 一行文本内容被反序列化成一个event。event的最大定义为2048字节，超过会被切割放到下一个event中。

7) 《enventBased策略下》，上传文件大小sizeUploadFileLocal 与 hdfs分割文件数量numFileCutdHDFS 关系：
	（timeBased策略下,rollCount=0，禁用）
	上传文件大小 sizeUploadFileLocal = 522k
	参数设置：
		maxLineLength = 204800  # 20k
		rollCount = 2
	hdfs分割文件单个大小 sizePerFileHDFS = maxLineLength * rollCount 
		if sizePerFileHDFS > siezeUploadFileLocal:
			numFileCutdHDFS = 1
			sizeFileHDFS = sizePerFileHDFS
		else:
			numFileCutdHDFS = math.ceil(sizeUploadFileLocal / sizePerFileHDFS)
			sizeFileListsHDFS[0:-1] = sizePerFileHDFS
			sizeFileListsHDFS[-1] = sizeUploadFileLocal - sizePerFileHDFS * (numFileCutdHDFS - 1)
	eg.
	sizeUploadFileLocal = 522k, maxLineLength = 20k, rollCount = 2  --->	numFileCutdHDFS = 14 (13*40k + 2.15k)
	sizeUploadFileLocal = 522k, maxLineLength = 20k, rollCount = 20 --->	numFileCutdHDFS = 2  (400k + 122.5k)
	......

8) hdfs需要event的header里要带时间戳，如果没有可以通过设置hdfs.useLocalTimeStamp=true来使用本地时间戳

9) taildir实时监控目录下的多个文件追加或监控文件新增
	1) 监控一个目录下的多个文件夹可以使用正则来匹配一批相同规则的文件，不同规则的文件若监控的话需要另起一个filegroups的子对象如f3
	2) 若果变更了部分监控文件的匹配规则，启动flume监控日志后，position中这部分旧匹配规则文件的position信息会被删除，被新匹配规则文件的position信息覆盖，
	   没变更匹配规则的文件position信息依旧会被保留；

10) hdfs sink中的参数batchSize不能小于channel的transactionCapacity：
	sink的batchsize是sink会一次从channel中取多少个event去发送，而这个发送是要最终以事务的形式去发送的，因此这个batchsize的event会传送到一个事务的缓存队列中（takeList），这是一个双向队列，这个队列可以在事务失败时进行回滚（也就是把取出来的数据吐memeryChannel的queue中），它的初始大小就是transactionCapacity定义的大小，
	比如：bathSize=100, transactionCapacity=10, 当100个events被发送到容量为10的缓存队列时，第11个event无没有位置可以存放，因此会报错；
	(batchSize设定：event在1k左右时，设置为500-1000合适)

11) spooldir source可以通过设置删除策略 deletePolicy 来处理重名文件
	Property Name 	Default 	Description
	deletePolicy 	never 		When to delete completed files: never or immediate

12) channelSelector类型中的复制Replicating和多路复用multiplexing ---> channel端
	Replicating缺陷：
		a) 每增加一个下游使用者，需要对应增加一个sink和一个channel；
		b) 且不具备动态增加的功能 （flume运行状态下无法增加，只能先停掉flume才能增加）
	Multiplexing：
		(1) 主要用于数据分类，需要结合拦截器Interpretor使用，拦截器增加header头信息

13) 负载均衡load_balance和故障转移failover ---> sinkProcessor，sink组 端
	sink组的策略类型：default, failover, load_balance
	failover:
		a1.sinkgroups = g1
		a1.sinkgroups.g1.sinks = k1 k2
		a1.sinkgroups.g1.processor.type = failover
		a1.sinkgroups.g1.processor.priority.k1 = 5		# priority 优先级，value大的优先级高
		a1.sinkgroups.g1.processor.priority.k2 = 10
		a1.sinkgroups.g1.processor.maxpenalty = 10000
	load_balance:
		a1.sinkgroups = g1
		a1.sinkgroups.g1.sinks = k1 k2
		a1.sinkgroups.g1.processor.type = load_balance
		a1.sinkgroups.g1.processor.backoff = true
		a1.sinkgroups.g1.processor.selector = random		# round_robin(轮循) or random(随机)

14) memoryChannel和fileChannel
	memoryChannel：存储在“JVM的堆内存“中，内存运算，速度快，不过数据会有一定的丢失；
	fileChannel：磁盘保存，速度慢，数据不易丢失
		(1) 通过配置多dir可以增大吞吐量 (一个dir对应一个磁盘)
		(2) checkpointDir 和 backupCheckpointDir要配置在不同的磁盘中 (df -h 命令查看磁盘情况)

15) hdfs.rollInterval, hdfs.rollSize, hdfs.rollCount默认设置会产生小文件问题，大量小文件对hadoop集群性能和拓展性有很大影响：
	(1) 占用NameNode大量内存:
		每个文件均按块存储，每个块的元数据存储在NameNode的内存中，因此HDFS存储小文件会非常低效。因为大量的小文件会耗尽NameNode中的大部分内存；
		
	(2) 磁盘寻址会变慢，降低吞吐量；
		数据块在硬盘为连续存储，对于普通SATA盘，随机寻址较慢，如果块设置过小，一个文件的块总数会越多，意味着磁盘盘寻址时间会加长，自然吞吐量无法满足要求；如果块设置过大，一方面对于普通盘来说IO性能也比较差，加载时会很慢，另一方面，块过大，对于多副本来说，在副本出问题时，系统恢复时间越长。
		
	(3) 多少个小文件就会开启多少个map任务，浪费大量资源：
		从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能

16) hdfs.round, hdfs.roundValue, hdfs.roundUnit如何控制分区？
	hdfs.round主要影响除了%t的其他所有时间表达式，xxxxx

17) flume一般生产环境分为2层或3层，原因是什么？单层会有什么问题？




PROBLEMS:
1) windows共享目录挂载到linux后，由于权限问题，flume-ng监控会出租哦，permission denied, 将共享文件夹的文件转移到centos7的路径后，ok!
2) flume的SLF4J日志jar包与hadoop的SLF4J日志jar包版本冲突，
	SLF4J: Class path contains multiple SLF4J bindings.
	SLF4J: Found binding in [jar:file:/usr/local/flume/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
	SLF4J: Found binding in [jar:file:/usr/hdp/2.6.3.0-235/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
   解决方案：
		flume中的 slf4j-log4j12-x.x.x.jar 包更名备份(使用hdfs-sink时更名，不适用hdfs-sink时再改回原jar包名)
3) taildir-flume-logger 中，设置如下filegroups.f1的监控文件：
		a1.sources.r1.filegroups.f1 = /usr/local/flume/files/file1.txt
		a1.sources.r1.filegroups.f1 = /usr/local/flume/files/spindle-info-20200116.log
	会导致 file1.txt 会被 spindle-info-20200116.log 覆盖掉，在 position.json 中记录的只有 spindle-info-20200116.log 的信息，没有 file1.txt的信息
	---> 说明 见 NOTES的9)所述

UPDATE:
1) 实现在每天的0点滚动文件
	refer: https://blog.csdn.net/weixin_33738578/article/details/86025675
	由于官方的1.8版本hdfs-sink不能在每天的0点滚动文件，所以修改了flume-hdfs-sink源码。
	flume-hdfs-sink中修改了HDFSEventSink.java文件，其他文件未改动.
	timeRollerFlag:
		默认值： day,可以设置minutes, hour, day .minutes 每分钟滚动文件,hour 每小时滚动文件,day 每天0点滚动文件
	hdfsSink的配置如下：
		paas2.sinks.k1.type = hdfs
		paas2.sinks.k1.hdfs.path = hdfs://ns1/user/hive/warehouse/dw_stg.db/tg_paas_business/dt=%Y%m%d
		paas2.sinks.k1.hdfs.filePrefix = paas_business_%Y%m%d
		paas2.sinks.k1.hdfs.fileSuffix = .COMPLITED
		paas2.sinks.k1.hdfs.rollInterval = 0
		paas2.sinks.k1.hdfs.rollCount = 0
		paas2.sinks.k1.hdfs.round = false
		paas2.sinks.k1.hdfs.roundValue=12
		paas2.sinks.k1.hdfs.roundUnit=hour
		paas2.sinks.k1.hdfs.rollSize = 134217728
		paas2.sinks.k1.hdfs.fileType = DataStream
		paas2.sinks.k1.hdfs.timeRollerFlag=day		# 滚动策略，每天0点滚动文件

	编译后的jar包flume-hdfs-sink-1.8.0.jar.modify_0000_rollFile.bak 替换flume/lib下的 flume-hdfs-sink-1.8.0.jar
	

	
	